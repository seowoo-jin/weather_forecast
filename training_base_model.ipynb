{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(in_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = h\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from utils.weather_api import WeatherApi\n",
    "from utils.common_function import splitData\n",
    "from enums.enums import Model, Date, Data, Rmse\n",
    "\n",
    "weatherApi = WeatherApi();\n",
    "area = 'Swanton_OH'\n",
    "# Swanton_OH\n",
    "X, y = weatherApi.get_weather_data_from_excel(area)\n",
    "X_train, X_test, y_train, y_test = splitData(X, y, 365)\n",
    "\n",
    "# MinMaxScaler 적용\n",
    "scaler = MinMaxScaler()\n",
    "if 'date' in X_train.columns:\n",
    "    X_train = X_train.drop(columns=['date'])\n",
    "    X_test = X_test.drop(columns=['date'])\n",
    "if 'date' in y_train.columns[0]:\n",
    "    y_train = y_train.drop(columns=[y_train.columns[0]])\n",
    "    y_test = y_test.drop(columns=[y_test.columns[0]])\n",
    "\n",
    "# MinMaxScaler 적용\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "date_range = pd.date_range(start='2023-08-01', end='2024-07-30')\n",
    "date_df = pd.DataFrame(date_range, columns=['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    params = {\n",
    "        'n_estimators':(100, 101),\n",
    "        'max_depth' : (16, 17),\n",
    "        'min_samples_leaf' : (5,6),\n",
    "        'min_samples_split' : (5,6)\n",
    "    }\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    rf_model = GridSearchCV(estimator=rf, param_grid=params, cv=10, n_jobs=-1)\n",
    "    rf_model_result = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    rf_best_model = rf_model_result.best_estimator_\n",
    "    rf_predict = rf_best_model.predict(X_test_scaled)\n",
    "\n",
    "    data_to_save = {\n",
    "        Model.MODEL.value: rf_best_model,\n",
    "        Data.TRAIN_INPUT_DATA.value: X_train_scaled,\n",
    "        Data.TRAIN_OUTPUT_DATA.value: X_test_scaled,\n",
    "        Data.VALID_INPUT_DATA.value: [],\n",
    "        Data.VALID_OUTPUT_DATA.value: [],\n",
    "        Data.TEST_INPUT_DATA.value: y_train,\n",
    "        Data.TEST_OUTPUT_DATA.value: y_test,\n",
    "        Data.PREDICTED_OUTPUT_DATA.value: pd.DataFrame(rf_predict, columns=y_test.columns),\n",
    "        Rmse.BEST_RMSE.value: math.sqrt(mean_squared_error(rf_predict, y_test.to_numpy())),\n",
    "        Date.DATE.value: date_df\n",
    "    }\n",
    "\n",
    "    path = f'result_model/{area}'\n",
    "    file_path = f'{path}/RF_model_with_{area}.pkl'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing start with fold in 0, i in 128 and j in 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     60\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 61\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Early stopping\u001b[39;00m\n\u001b[1;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/torch/optim/adam.py:380\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    379\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 380\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    383\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_mlp_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_mlp_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train_mlp_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test_mlp_scaled, dtype=torch.float32), torch.tensor(y_test.values, dtype=torch.float32))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "    min_i_value = 0\n",
    "    min_j_value = 0\n",
    "    mlp_best_rmse = 100\n",
    "    # 최적 모델 구성에서 사용된 데이터 저장 변수 초기화\n",
    "    best_train_input_data = None\n",
    "    best_train_output_data = None\n",
    "    best_valid_input_data = None\n",
    "    best_valid_output_data = None\n",
    "    best_test_input_data = None\n",
    "    best_test_output_data = None\n",
    "    best_valid_predictions = None\n",
    "    best_test_predictions = None\n",
    "    mlp_best_rmse = float('inf')\n",
    "\n",
    "    moving_valid_rmse = []\n",
    "    moving_test_rmse = []\n",
    "    mlp_best_model = None\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(X_train_mlp_scaled)):\n",
    "\n",
    "        # Split the data\n",
    "        X_train_fold, X_valid_fold = X_train_tensor[train_index], X_train_tensor[valid_index]\n",
    "        y_train_fold, y_valid_fold = y_train_tensor[train_index], y_train_tensor[valid_index]\n",
    "\n",
    "        # TensorDataset, DataLoader로 변경\n",
    "        train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "        valid_dataset = TensorDataset(X_valid_fold, y_valid_fold)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "        for i in [128, 256, 512]:\n",
    "            for j in  [4, 8, 16, 32]:\n",
    "                if(i > j):\n",
    "                    model = MLP(input_size=len(X_train.columns), hidden_layers=[i, j], output_size=len(y_train.columns))\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "                    num_epochs = 1000\n",
    "                    best_loss = float('inf')\n",
    "                    epochs_no_improve = 0\n",
    "                    early_stop = False\n",
    "                    print(f'processing start with fold in {fold}, i in {i} and j in {j}')\n",
    "                    for epoch in range(num_epochs):\n",
    "                        model.train()\n",
    "                        for inputs, labels in train_loader:\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        # Early stopping\n",
    "                        model.eval()\n",
    "                        val_loss = 0\n",
    "                        with torch.no_grad():\n",
    "                            for inputs, labels in valid_loader:\n",
    "                                outputs = model(inputs)\n",
    "                                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "                        val_loss /= len(valid_loader)\n",
    "\n",
    "                        # print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss}')\n",
    "\n",
    "                        # 연속적으로 best_loss보다 개선되지 않는 횟수\n",
    "                        if val_loss < best_loss:\n",
    "                            best_loss = val_loss\n",
    "                            epochs_no_improve = 0\n",
    "                        else:\n",
    "                            epochs_no_improve += 1\n",
    "                        # patience만큼 개선이 없으면 종료\n",
    "                        if epochs_no_improve >= 50:\n",
    "                            print(f'Early stopping after {epoch+1} epochs')\n",
    "                            early_stop = True\n",
    "                            break\n",
    "                                # Model evaluation\n",
    "\n",
    "                    model.eval()\n",
    "                    valid_predictions = []\n",
    "                    valid_true_values = []\n",
    "                    with torch.no_grad():\n",
    "                        for inputs, labels in valid_loader:\n",
    "                            outputs = model(inputs)\n",
    "                            valid_predictions.append(outputs.numpy())\n",
    "                            valid_true_values.append(labels.numpy())\n",
    "\n",
    "                    valid_predictions = np.vstack(valid_predictions)\n",
    "                    valid_true_values = np.vstack(valid_true_values)\n",
    "\n",
    "                    valid_rmse = mean_squared_error(valid_true_values, valid_predictions, squared=True)\n",
    "                    moving_valid_rmse.append(valid_rmse)\n",
    "                    if(mlp_best_rmse > valid_rmse):\n",
    "                        mlp_best_model = model\n",
    "                        mlp_best_rmse = valid_rmse\n",
    "                        test_predictions = []\n",
    "                        test_true_values = []\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            for inputs, labels in test_loader:\n",
    "                                outputs = model(inputs)\n",
    "                                test_predictions.append(outputs.numpy())\n",
    "                                test_true_values.append(labels.numpy())\n",
    "                        test_predictions = np.vstack(test_predictions)\n",
    "                        test_true_values = np.vstack(test_true_values)\n",
    "                        test_rmse = mean_squared_error(test_true_values, test_predictions, squared=False)\n",
    "                        best_train_input_data = pd.DataFrame(train_dataset.tensors[0].numpy(), columns=[f'{i}' for i in X.columns])\n",
    "                        best_train_output_data = pd.DataFrame(train_dataset.tensors[1].numpy(), columns=[f'{i}' for i in y.columns])\n",
    "                        best_valid_input_data = pd.DataFrame(valid_dataset.tensors[0].numpy(), columns=[f'{i}' for i in X.columns])\n",
    "                        best_valid_output_data = pd.DataFrame(valid_dataset.tensors[1].numpy(), columns=[f'{i}' for i in y.columns])\n",
    "                        best_test_input_data = pd.DataFrame(test_dataset.tensors[0].numpy(), columns=[f'{i}' for i in X.columns])\n",
    "                        best_test_output_data = pd.DataFrame(test_dataset.tensors[1].numpy(), columns=[f'{i}' for i in y.columns])\n",
    "                        best_test_predictions = pd.DataFrame(test_predictions, columns=[f'{i}' for i in y.columns])\n",
    "\n",
    "                        print(f'first node : {i}, snd node : {j},  fold : {fold}, valid_rmse : {valid_rmse}, test_rmse : {test_rmse}')\n",
    "                    print(f'processing end with  fold in {fold} i in {i} and j in {j}')\n",
    "\n",
    "    data_to_save = {\n",
    "        Model.MODEL.value: mlp_best_model,\n",
    "        Data.TRAIN_INPUT_DATA.value: best_train_input_data,\n",
    "        Data.TRAIN_OUTPUT_DATA.value: best_train_output_data,\n",
    "        Data.VALID_INPUT_DATA.value: best_valid_input_data,\n",
    "        Data.VALID_OUTPUT_DATA.value: best_valid_output_data,\n",
    "        Data.TEST_INPUT_DATA.value: best_test_input_data,\n",
    "        Data.TEST_OUTPUT_DATA.value: best_test_output_data,\n",
    "        Data.PREDICTED_OUTPUT_DATA.value: best_test_predictions,\n",
    "        Rmse.BEST_RMSE: mlp_best_rmse,\n",
    "        Date.DATE.value: date_df,\n",
    "    }\n",
    "\n",
    "    path = f'result_model/{area}'\n",
    "    file_path = f'{path}/MLP_model_with_{area}.pkl'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    param_grid = {\n",
    "        'estimator__n_estimators': [50],\n",
    "        'estimator__learning_rate': [0.1],\n",
    "        'estimator__estimator__max_depth': [5]\n",
    "    }\n",
    "\n",
    "    base_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor())\n",
    "    fit_model = MultiOutputRegressor(base_ada)\n",
    "\n",
    "    adaboost_model = GridSearchCV(estimator=fit_model, param_grid=param_grid, cv=10)\n",
    "    adaboost_model_result = adaboost_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    Adaboost_best_model = adaboost_model_result.best_estimator_\n",
    "\n",
    "    Adaboost_predictions = Adaboost_best_model.predict(X_test)\n",
    "\n",
    "    # Calculate and print RMSE\n",
    "    Adaboost_best_rmse = math.sqrt(mean_squared_error(y_test, Adaboost_predictions))\n",
    "\n",
    "    data_to_save = {\n",
    "        Model.MODEL.value: Adaboost_best_model,\n",
    "        Data.TRAIN_INPUT_DATA.value: X_train_scaled,\n",
    "        Data.TRAIN_OUTPUT_DATA.value: X_test_scaled,\n",
    "        Data.VALID_INPUT_DATA.value: [],\n",
    "        Data.VALID_OUTPUT_DATA.value: [],\n",
    "        Data.TEST_INPUT_DATA.value: y_train,\n",
    "        Data.TEST_OUTPUT_DATA.value: y_test,\n",
    "        Data.PREDICTED_OUTPUT_DATA.value: Adaboost_predictions,\n",
    "        Rmse.BEST_RMSE: Adaboost_best_rmse,\n",
    "        Date.DATE.value: date_df,\n",
    "    }\n",
    "\n",
    "    path = f'result_model/{area}'\n",
    "    file_path = f'{path}/Adaboost_model_with_{area}.pkl'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py\", line 1377, in fit\n",
      "    super()._fit(\n",
      "  File \"/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py\", line 269, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Some value(s) of y are negative which is not allowed for Poisson regression.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/sw.jin/Documents/projects/wf/workspace/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-44.44392407 -44.44392407 -44.44392407 -44.44392407 -25.7122748\n",
      " -25.7122748  -25.7122748  -25.7122748  -20.61790318 -20.64185824\n",
      " -20.57569319 -20.28776867 -19.61915084 -19.31267271 -19.20104182\n",
      " -18.97172364 -20.12316122 -19.43795787 -19.08160751 -18.90045055\n",
      " -20.47862086 -19.42074277 -19.08696504 -18.89197868 -42.0365472\n",
      " -42.0365472  -42.0365472  -42.0365472  -25.64052761 -25.64052761\n",
      " -25.64052761 -25.64052761 -19.34858054 -19.36540657 -19.31829202\n",
      " -19.47685967 -18.58002054 -18.50721833 -18.3234001  -18.25669195\n",
      " -20.05702302 -18.77346069 -18.44514462 -18.18972634 -20.88697123\n",
      " -18.77682783 -18.45453623 -18.17072623          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan -41.61534739 -41.61534739 -41.61534739\n",
      " -41.61534739 -24.50107512 -24.50107512 -24.50107512 -24.50107512\n",
      " -19.33158426 -19.1747335  -19.19317051 -19.14402054 -18.44046894\n",
      " -17.81364256 -17.53918426 -17.38987555 -19.50280958 -18.02322205\n",
      " -17.55198784 -17.30029888 -20.23029013 -18.21272837 -17.57912788\n",
      " -17.30694328]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipe_tree = make_pipeline(DecisionTreeRegressor(random_state=2021))\n",
    "    # 트리의 파라미터 키값 확인\n",
    "    pipe_tree.get_params().keys()\n",
    "\n",
    "    param_range1 = [1, 3, 5, 7, 9, 11]\n",
    "    param_range2 = [5, 10, 15, 20]\n",
    "    param_range3 = ['friedman_mse', 'absolute_error', 'poisson', 'squared_error'] # 'explained_variance'도 가능\n",
    "\n",
    "    param_grid = [{'decisiontreeregressor__max_depth': param_range1,\n",
    "                'decisiontreeregressor__min_samples_leaf': param_range2,\n",
    "                'decisiontreeregressor__criterion': param_range3}]\n",
    "\n",
    "    decisionTree_model = GridSearchCV(\n",
    "        estimator = pipe_tree,\n",
    "        param_grid = param_grid,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        n_jobs= -1,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "    decisionTree_model_result = decisionTree_model.fit(X_train_scaled, y_train)\n",
    "    decisionTree_best_model = decisionTree_model_result.best_estimator_\n",
    "\n",
    "    decisionTree__pred = decisionTree_best_model.predict(X_test_scaled)\n",
    "\n",
    "    data_to_save = {\n",
    "        Model.MODEL.value: decisionTree_best_model,\n",
    "        Data.TRAIN_INPUT_DATA.value: X_train_scaled,\n",
    "        Data.TRAIN_OUTPUT_DATA.value: X_test_scaled,\n",
    "        Data.VALID_INPUT_DATA.value: [],\n",
    "        Data.VALID_OUTPUT_DATA.value: [],\n",
    "        Data.TEST_INPUT_DATA.value: y_train,\n",
    "        Data.TEST_OUTPUT_DATA.value: y_test,\n",
    "        Data.PREDICTED_OUTPUT_DATA.value: decisionTree__pred,\n",
    "        Rmse.BEST_RMSE: math.sqrt(mean_squared_error(decisionTree__pred, y_test)),\n",
    "        Date.DATE.value: date_df,\n",
    "    }\n",
    "\n",
    "    path = f'result_model/{area}'\n",
    "    file_path = f'{path}/DecisionTree_model_with_{area}.pkl'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTree 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE':3.407599731908794\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tunning_model = ExtraTreesRegressor()\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=tunning_model,\n",
    "        param_grid={\n",
    "            'n_estimators': range(100, 200, 100),\n",
    "            'max_features': range(10,20,10),\n",
    "            'min_samples_leaf': range(5,10,5),\n",
    "            'min_samples_split': range(5,10,5),\n",
    "        },\n",
    "        cv=10\n",
    "    )\n",
    "    extra_best_model_result = gsc.fit(X_train_scaled.to_numpy(), y_train.to_numpy())\n",
    "    extra_best_model = extra_best_model_result.best_estimator_\n",
    "\n",
    "    extra_best_prediction = extra_best_model.predict(X_test_scaled.to_numpy())\n",
    "\n",
    "    data_to_save = {\n",
    "        Model.MODEL.value: extra_best_model,\n",
    "        Data.TRAIN_INPUT_DATA.value: X_train_scaled,\n",
    "        Data.TRAIN_OUTPUT_DATA.value: X_test_scaled,\n",
    "        Data.VALID_INPUT_DATA.value: [],\n",
    "        Data.VALID_OUTPUT_DATA.value: [],\n",
    "        Data.TEST_INPUT_DATA.value: y_train,\n",
    "        Data.TEST_OUTPUT_DATA.value: y_test,\n",
    "        Data.PREDICTED_OUTPUT_DATA.value: extra_best_prediction,\n",
    "        Rmse.BEST_RMSE: math.sqrt(mean_squared_error(extra_best_prediction, y_test)),\n",
    "        Date.DATE.value: date_df,\n",
    "    }\n",
    "\n",
    "    path = f'result_model/{area}'\n",
    "    file_path = f'{path}/ExtraTree_model_with_{area}.pkl'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gradientBoosting = GradientBoostingRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 3],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    gradientBoosting_model_result = GridSearchCV(\n",
    "        MultiOutputRegressor(gradientBoosting),\n",
    "        param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    gradientBoosting_model_result.fit(X_train_scaled, y_train)\n",
    "    gradientBoosting_best_model = gradientBoosting_model_result.best_estimator_\n",
    "\n",
    "    gradientBoosting_predictions = gradientBoosting_best_model.predict(X_test_scaled)\n",
    "\n",
    "    # Save the results\n",
    "    data_to_save = {\n",
    "        Model.MODEL.value: gradientBoosting_best_model,\n",
    "        Data.TRAIN_INPUT_DATA.value: X_train_scaled,\n",
    "        Data.TRAIN_OUTPUT_DATA.value: y_train,\n",
    "        Data.VALID_INPUT_DATA.value: [],\n",
    "        Data.VALID_OUTPUT_DATA.value: [],\n",
    "        Data.TEST_INPUT_DATA.value: X_test_scaled,\n",
    "        Data.TEST_OUTPUT_DATA.value: y_test,\n",
    "        Data.PREDICTED_OUTPUT_DATA.value: gradientBoosting_predictions,\n",
    "        Rmse.BEST_RMSE: math.sqrt(mean_squared_error(y_test, gradientBoosting_predictions)),\n",
    "        Date.DATE.value: date_df,\n",
    "    }\n",
    "\n",
    "    path = f'result_model/{area}'\n",
    "    file_path = f'{path}/GradientBoosting_model_with_{area}.pkl'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    params = {\n",
    "        'max_depth':[5,7],\n",
    "        'min_child_weight':[1,3],\n",
    "        'colsample_bytree':[0.5,0.75]\n",
    "    }\n",
    "    tunning_model = xgboost.XGBRegressor()\n",
    "    xgboost_model = GridSearchCV(\n",
    "        estimator=tunning_model,\n",
    "        param_grid=params,\n",
    "        scoring='r2',\n",
    "        cv=10\n",
    "    )\n",
    "    xgboost_result = xgboost_model.fit(X_train_scaled, y_train)\n",
    "    xgboost_best_model = xgboost_result.best_estimator_\n",
    "    xgboost_predictions = xgboost_best_model.predict(X_test_scaled)\n",
    "\n",
    "    data_to_save = {\n",
    "        Model.MODEL.value: xgboost_best_model,\n",
    "        Data.TRAIN_INPUT_DATA.value: X_train_scaled,\n",
    "        Data.TRAIN_OUTPUT_DATA.value: X_test_scaled,\n",
    "        Data.VALID_INPUT_DATA.value: [],\n",
    "        Data.VALID_OUTPUT_DATA.value: [],\n",
    "        Data.TEST_INPUT_DATA.value: y_train,\n",
    "        Data.TEST_OUTPUT_DATA.value: y_test,\n",
    "        Data.PREDICTED_OUTPUT_DATA.value: xgboost_predictions,\n",
    "        Rmse.BEST_RMSE: math.sqrt(mean_squared_error(xgboost_predictions, y_test)),\n",
    "        Date.DATE.value: date_df,\n",
    "    }\n",
    "\n",
    "    path = f'result_model/{area}'\n",
    "    file_path = f'{path}/Xgboost_model_with_{area}.pkl'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
