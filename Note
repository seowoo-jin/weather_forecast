
예측 데이터 기간 : 2023-08-01 ~ 2024-07-30 까지의 데이터 예측

예측 모델 학습 순서 : 데이터 스플릿 -> 하이퍼 파라미터 최적화 -> 최적화된 하이퍼 파라미터를 이용해서 cross validation
예측 결과 분석 :
    - 월별 예측 결과 RMSE 표현
    - 전체 예측 결과 RMSE 표현
    - 계졀별 RMSE 표현
    - 모델 학습시 학습 되는 과정에서의 RMSE 값들 그래프로 표현
    - 최대온도, 최저온도, 풍속 세가지에 대해 각각 RMSE 표현

모델 종류
    - Adaboost -> 제거(학습 소요시간이 너무 오래 걸림)
    기본 모델 트리
        - Decision Tree
        - Extra Tree
    기본 모델 Layer
        - Multi Layer Perceptron
    앙상블 모델
        - Gradient boosting
        - Random Forest
        - Xgboost

앙상블 기법
    VotingRegressor: 여러 모델의 예측을 평균냄.
    StackingRegressor: 여러 모델의 예측을 기반으로 메타 모델을 학습시킴.
    BaggingRegressor: 단일 모델에 대해 부트스트랩 샘플을 사용하여 결과를 평균냄.

TODO
기존 모델을 합쳐서 더 나은 모델 검증(모델 개발) - 기상에측을 위한 앙상블 기법 제안
기존 mlp 모델보다 앙상블 모델이 좀더 나은 결과를 보여준다..
기존 모델을 기준으로 더나은 결과를 보여줌
모델 결합 근거(2, 3, 4...결합 가능) - 기상예측에 대해 서로의 단점을 완화해주는 모델들의 결합
기상예측의 데이터의 특징을 파악
모델별(비교 모델, 앙상블 모델) 비교 결과
단일모델에 비해 조금만 더 좋아도 됨
논문 구성 대략적으로 작성





1. Gradient Boosting
    기본 아이디어: 여러 약한 학습기(주로 결정 트리)를 순차적으로 학습시키며, 이전 학습기의 오차(잔여)를 다음 학습기가 보완하도록 학습합니다. 결과적으로, 학습기의 오차가 점진적으로 줄어들면서 모델의 성능이 향상됩니다.
    과정:
        각 학습 단계에서 잘못 예측한 부분을 보정하기 위해 가중치를 조정.
        새 모델이 추가될 때마다 이전 모델의 오류를 줄이는 방식으로 학습 진행.
    특징:
        순차적 학습: 한 모델이 학습을 완료한 후 다음 모델이 그 결과를 기반으로 학습.
        Overfitting 방지: 학습기가 너무 많으면 과적합이 발생할 수 있으므로 적절한 하이퍼파라미터 튜닝이 필요.
        느림: 단계적으로 학습하므로 비교적 느린 성능.
2. Random Forest
    기본 아이디어: 다수의 결정 트리를 학습시켜 각 트리의 예측 결과를 평균 내거나 투표를 통해 최종 예측을 도출합니다. 각 트리는 무작위로 샘플링된 데이터를 사용해 독립적으로 학습합니다.
    과정:
        데이터의 부트스트랩 샘플링(무작위 샘플링) 및 특징 무작위 선택을 통해 각 트리가 서로 다른 패턴을 학습하도록 함.
        각 트리의 예측 결과를 합산하여 최종 예측을 도출.
    특징:
        병렬 학습 가능: 각 트리가 독립적으로 학습하므로 병렬 처리가 가능해 학습이 빠름.
        Overfitting 방지: 다수의 트리가 서로 다른 샘플로 학습하므로 개별 트리의 과적합을 줄임.
        정확도 높음: 다수의 트리 예측 결과를 결합하므로 일반화 성능이 뛰어남.
3. XGBoost (Extreme Gradient Boosting)
    기본 아이디어: Gradient Boosting의 확장된 버전으로, 성능 최적화 및 속도 향상에 중점을 둔 앙상블 모델. Gradient Boosting의 순차적 학습 방식에 다양한 최적화 기능을 추가하여 더 빠르고, 더 정확한 예측을 만듭니다.
    과정:
        Gradient Boosting과 유사한 방식으로 학습하되, 학습 중에 정규화를 추가하여 과적합을 방지하고, 병렬 처리를 통해 속도를 높임.
    특징:
        Regularization (정규화): L1, L2 정규화를 사용하여 모델 복잡도를 조절, 과적합을 방지.
        빠른 학습 속도: 병렬 처리 및 계산 최적화를 통해 학습 속도가 매우 빠름.
        강력한 성능: 특성 중요도 계산, missing value 처리, 트리의 깊이 조절 등 다양한 기능 제공으로 높은 성능을 발휘.
차이점 요약:
    Gradient Boosting은 순차적으로 모델을 학습하여 오차를 줄이는 데 중점을 둠.
    Random Forest는 병렬로 여러 트리를 학습시켜 예측 결과를 종합.
    XGBoost는 Gradient Boosting을 확장한 모델로, 정규화와 병렬 처리를 통해 더 빠르고 과적합 방지 기능이 추가됨.
정리
    Gradient Boosting은 이전 모델의 오차를 보완하는 방식으로 성능을 높이지만 느릴 수 있음.
    Random Forest는 여러 트리를 병렬 학습하며 예측 성능이 우수하고 빠름.
    XGBoost는 Gradient Boosting의 성능을 강화한 모델로, 빠르고 강력한 최적화 기능을 제공.